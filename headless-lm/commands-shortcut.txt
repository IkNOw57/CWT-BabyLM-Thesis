cd C:\Course_Data\Thesis-voorbereiding\headless-lm\
cd C:\Course_Data\evaluation-pipeline-2024

## PREPROCESSING
python preprocess.py --config=configs\preprocess_baby_lm.json


## PUBLISH TO HUGGINGFACE
python hf_publisher.py --hf_name InoWouw/BabyLM-encoder --model_ckpt your_model.ckpt --mode mlm

python hf_publisher.py --hf_name InoWouw/BabyLM-decoder --model_ckpt "C:\Course_Data\Thesis-voorbereiding\headless-lm\checkpoints\checkpoints_last\epoch=36-step=15000.ckpt" --mode add_head


## MLM TRAINING
(BASIC): python mlm_headless.py --config configs\mlm_headless_baby_lm.json --num_nodes 1 --global_bs 256 --gpu_bs 256  --dataset dataset_storage\baby-lm-small.hf   --hf_tokenizer bert-base-uncased --hf_path google-bert/bert-base-cased --model_max_seq_len 2048 --run_name checkpoints --saved_ckpt_path checkpoints

(DESKTOP-Ino): python mlm_headless.py --config configs\mlm_headless_baby_lm.json --num_nodes 1 --global_bs 16 --gpu_bs 16  --dataset dataset_storage\baby-lm-small.hf   --hf_tokenizer bert-base-uncased --hf_path google-bert/bert-base-cased --model_max_seq_len 512 --run_name checkpoints --saved_ckpt_path checkpoints --ckpt_every 2000 --max_epoch 10


## GLUE FINETUNING
python glue_finetuning_unpublished.py --model_ckpt .\checkpoints\checkpoints_last\epoch=36-step=15000.ckpt --mode mlm

## BLIMP EVAL
git bash naar evaluation pipeline: vergeet niet de Conda omgeving te activeren
./eval_blimp.sh InoWouw/BabyLM-encoder
./eval_ewok.sh InoWouw/BabyLM-encoder
cd /mnt/c/Course_Data/evaluation-pipeline-2024
source /mnt/c/Users/Ino/anaconda3/etc/profile.d/conda.sh 



