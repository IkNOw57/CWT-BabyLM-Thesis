{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc9383d",
   "metadata": {},
   "source": [
    "# Unsupervised Subword Tokenizers vs. Morphology\n",
    "\n",
    "Let's explore how unsupervised tokenizers, commonly used in Deep Learning, relate to the more linguistic aspects of Morphology. Your task is to tweek the code in order to see if subword tokenization could be a proxy for real morphological analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbfe369",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Things you may need to do before running the code\n",
    "\n",
    "### Install NLTK and Tokenizers packages:\n",
    "\n",
    "```\n",
    "pip install tokenizers\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "### Download the Brown Corpus from NLTK\n",
    "\n",
    "\n",
    "```\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c323e2f-fccf-4f0a-b72d-290b1bc2c097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tokenizers\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b975bd41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "corpus_f = open(\"brown-corpus.txt\", \"w+\")\n",
    "\n",
    "# count = 0\n",
    "# vocab = set()\n",
    "for s in brown.sents():\n",
    "    corpus_f.write(\" \".join(s) + '\\n')\n",
    "    \n",
    "#     words =str(s).split()\n",
    "#     count += len(words)\n",
    "#     vocab.update(words)\n",
    "\n",
    "# print(\"No. of words:\", count)\n",
    "# print(\"No. of unique words:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec599103",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eed95d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66857c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 100   # You should be playing with this threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f038dd5",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding (BPE)  tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc07c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BPE_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, \n",
    "                     special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "BPE_tokenizer.pre_tokenizer = Whitespace()    # This is optional...\n",
    "\n",
    "files = [\"brown-corpus.txt\"]\n",
    "\n",
    "BPE_tokenizer.train(files, trainer)\n",
    "\n",
    "BPE_tokenizer.save(\"BPE-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97fa91",
   "metadata": {},
   "source": [
    "# Wordpiece tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355f7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WP_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "WP_trainer = WordPieceTrainer(vocab_size=VOCAB_SIZE,\n",
    "                              special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "WP_tokenizer.pre_tokenizer = Whitespace()    # This is optional...\n",
    "\n",
    "files = [\"brown-corpus.txt\"]\n",
    "\n",
    "WP_tokenizer.train(files, WP_trainer)\n",
    "\n",
    "WP_tokenizer.save(\"WP-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc231d50",
   "metadata": {},
   "source": [
    "#  Unigram tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da5b6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "UG_tokenizer = Tokenizer(Unigram())\n",
    "\n",
    "UG_trainer = UnigramTrainer(vocab_size=VOCAB_SIZE,\n",
    "                            unk_token=\"<UNK>\",\n",
    "                            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "UG_tokenizer.pre_tokenizer = Whitespace()    # This is optional... \n",
    "\n",
    "files = [\"brown-corpus.txt\"]\n",
    "\n",
    "UG_tokenizer.train(files, UG_trainer)\n",
    "\n",
    "UG_tokenizer.save(\"UG-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098cdf6a",
   "metadata": {},
   "source": [
    "# Let's compare the tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0e11d",
   "metadata": {},
   "source": [
    "Your task here will be to use a small evaluation corpus to test how the different algorithms perform against one another, while varying the size of the vocabulary above.\n",
    "\n",
    "Feel free to add other words see how they are segmented (but you need to provide a gold segmentation for it to work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045310e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some data extracted from https://github.com/sigmorphon/2022SegmentationST\n",
    "test_corpus = [\n",
    "    [\"assistant\", [\"assist\",\"ant\"]],\n",
    "    [\"assistants\", [\"assist\",\"ant\",\"s\"]],\n",
    "    [\"assist\", [\"assist\"]],\n",
    "    [\"assisted\",[\"assist\",\"ed\"]],\n",
    "    [\"assisting\", [\"assist\",\"ing\"]],\n",
    "    [\"assistance\",[\"assist\", \"ance\"]],\n",
    "    [\"assistive\", [\"assist\",\"ive\"]],\n",
    "    [\"assistful\", [\"assist\",\"ful\"]],\n",
    "    [\"assister\", [\"assist\",\"er\"]],\n",
    "    [\"unassisted\", [\"un\",\"assist\",\"ed\"]],\n",
    "    [\"coassistance\", [\"co\",\"assist\",\"ance\"]],\n",
    "    [\"coassists\", [\"co\",\"assist\",\"s\"]],\n",
    "    [\"overassisting\",[\"over\",\"assist\",\"ing\"]],\n",
    "    [\"entaming\", [\"en\", \"tame\", \"ing\"]],\n",
    "    [\"hoarders\", [\"hoard\", \"er\", \"s\"]],\n",
    "    [\"visitorship\", [\"visit\",\"or\",\"ship\"]],\n",
    "    [\"reorganises\", [\"re\",\"organise\",\"s\"]],\n",
    "    [\"wargamer\", [\"war\",\"game\",\"er\"]],               \n",
    "    [\"encodability\", [\"en\",\"code\",\"ability\"]],\n",
    "    [\"healthy\", [\"health\",\"y\"]],\n",
    "    [\"buildings\", [\"build\",\"ing\",\"s\"]],\n",
    "    [\"socioeconomy\", [\"socio\",\"economy\"]],\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80407821-6e7d-4358-9a8e-84216ead8e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for instance in test_corpus:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7ce00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_wp, count_bpe, count_ug = 0, 0, 0\n",
    "\n",
    "report = \"\"\n",
    "for word, morphs  in test_corpus:\n",
    "    \n",
    "    wp = WP_tokenizer.decode(WP_tokenizer.encode(word).ids).replace(\"#\",'').split()\n",
    "    bpe = BPE_tokenizer.decode(BPE_tokenizer.encode(word).ids).split()\n",
    "    ug = UG_tokenizer.decode(UG_tokenizer.encode(word).ids).split()\n",
    "\n",
    "    if wp==morphs:\n",
    "        count_wp += 1\n",
    "    if bpe==morphs:\n",
    "        count_bpe += 1\n",
    "    if ug==morphs:\n",
    "        count_ug += 1\n",
    "\n",
    "        \n",
    "    report = report + \"GOLD: \" + \" \".join(morphs) + \"\\n\"\n",
    "\n",
    "    report = report + \"Wordpiece: \" + WP_tokenizer.decode(WP_tokenizer.encode(word).ids).replace(\"#\",'') + \"\\n\"\n",
    "\n",
    "    report = report + \"BPE: \" + BPE_tokenizer.decode(BPE_tokenizer.encode(word).ids) + \"\\n\"\n",
    "\n",
    "    report = report + \"Unigram: \" + UG_tokenizer.decode(UG_tokenizer.encode(word).ids) + \"\\n\"\n",
    "    \n",
    "    report = report + \"------------------------------------------\\n\"\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------\")\n",
    "print(\"RESULTS:\")\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Wordpiece:\", count_wp)\n",
    "print(\"BPE:\", count_bpe)\n",
    "print(\"Unigram:\", count_ug)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"\\n\\n\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330384a6-81d6-4740-88c5-f93b67b38716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c90d4-bfe8-4663-8d61-290835f4d5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbaaad3-5faa-4334-be1b-0990d5c82301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
