{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc9383d",
   "metadata": {},
   "source": [
    "# Unsupervised Subword Tokenizers vs. Morphology\n",
    "\n",
    "Let's explore how unsupervised tokenizers, commonly used in Deep Learning, relate to the more linguistic aspects of Morphology. Your task is to tweek the code in order to see if subword tokenization could be a proxy for real morphological analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbfe369",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Things you may need to do before running the code\n",
    "\n",
    "### Install NLTK and Tokenizers packages:\n",
    "\n",
    "```\n",
    "pip install tokenizers\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "### Download the Brown Corpus from NLTK\n",
    "\n",
    "\n",
    "```\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c323e2f-fccf-4f0a-b72d-290b1bc2c097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tokenizers\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b975bd41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ino\\anaconda3\\envs\\gpu_test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from datasets import load_dataset   \n",
    "corpus_f = open(\"BabyLM-corpus-strict-small.txt\", \"w+\")\n",
    "corpus = load_dataset(\"cambridge-climb/BabyLM\",\"strict_small\",trust_remote_code = True)\n",
    "# corpus['train']['text']\n",
    "# count = 0\n",
    "# # vocab = set()\n",
    "for s in corpus['train']['text']:\n",
    "   corpus_f.write(\"\".join(s) + '\\n')\n",
    "    \n",
    "#     words =str(s).split()\n",
    "#     count += len(words)\n",
    "#     vocab.update(words)\n",
    "\n",
    "# print(\"No. of words:\", count)\n",
    "# print(\"No. of unique words:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec599103",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6eed95d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66857c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000   # You should be playing with this threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f038dd5",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding (BPE)  tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dc07c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BPE_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, \n",
    "                     special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "BPE_tokenizer.pre_tokenizer = Whitespace()    # This is optional...\n",
    "\n",
    "files = [\"BabyLM-corpus-strict-small.txt\"]\n",
    "\n",
    "BPE_tokenizer.train(files, trainer)\n",
    "\n",
    "BPE_tokenizer.save(\"tokenizers/BPE-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97fa91",
   "metadata": {},
   "source": [
    "# Wordpiece tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0355f7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WP_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "WP_trainer = WordPieceTrainer(vocab_size=VOCAB_SIZE,\n",
    "                              special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "WP_tokenizer.pre_tokenizer = Whitespace()    # This is optional...\n",
    "\n",
    "files = [\"BabyLM-corpus-strict-small.txt\"]\n",
    "\n",
    "WP_tokenizer.train(files, WP_trainer)\n",
    "\n",
    "WP_tokenizer.save(\"tokenizers/WP-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc231d50",
   "metadata": {},
   "source": [
    "#  Unigram tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da5b6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "UG_tokenizer = Tokenizer(Unigram())\n",
    "\n",
    "UG_trainer = UnigramTrainer(vocab_size=VOCAB_SIZE,\n",
    "                            unk_token=\"<UNK>\",\n",
    "                            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "UG_tokenizer.pre_tokenizer = Whitespace()    # This is optional... \n",
    "\n",
    "files = [\"BabyLM-corpus-strict-small.txt\"]\n",
    "\n",
    "UG_tokenizer.train(files, UG_trainer)\n",
    "\n",
    "UG_tokenizer.save(\"tokenizers/UG-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098cdf6a",
   "metadata": {},
   "source": [
    "# Let's compare the tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0e11d",
   "metadata": {},
   "source": [
    "Your task here will be to use a small evaluation corpus to test how the different algorithms perform against one another, while varying the size of the vocabulary above.\n",
    "\n",
    "Feel free to add other words see how they are segmented (but you need to provide a gold segmentation for it to work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2045310e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some data extracted from https://github.com/sigmorphon/2022SegmentationST\n",
    "test_corpus = [\n",
    "    [\"assistant\", [\"assist\",\"ant\"]],\n",
    "    [\"assistants\", [\"assist\",\"ant\",\"s\"]],\n",
    "    [\"assist\", [\"assist\"]],\n",
    "    [\"assisted\",[\"assist\",\"ed\"]],\n",
    "    [\"assisting\", [\"assist\",\"ing\"]],\n",
    "    [\"assistance\",[\"assist\", \"ance\"]],\n",
    "    [\"assistive\", [\"assist\",\"ive\"]],\n",
    "    [\"assistful\", [\"assist\",\"ful\"]],\n",
    "    [\"assister\", [\"assist\",\"er\"]],\n",
    "    [\"unassisted\", [\"un\",\"assist\",\"ed\"]],\n",
    "    [\"coassistance\", [\"co\",\"assist\",\"ance\"]],\n",
    "    [\"coassists\", [\"co\",\"assist\",\"s\"]],\n",
    "    [\"overassisting\",[\"over\",\"assist\",\"ing\"]],\n",
    "    [\"entaming\", [\"en\", \"tame\", \"ing\"]],\n",
    "    [\"hoarders\", [\"hoard\", \"er\", \"s\"]],\n",
    "    [\"visitorship\", [\"visit\",\"or\",\"ship\"]],\n",
    "    [\"reorganises\", [\"re\",\"organise\",\"s\"]],\n",
    "    [\"wargamer\", [\"war\",\"game\",\"er\"]],               \n",
    "    [\"encodability\", [\"en\",\"code\",\"ability\"]],\n",
    "    [\"healthy\", [\"health\",\"y\"]],\n",
    "    [\"buildings\", [\"build\",\"ing\",\"s\"]],\n",
    "    [\"socioeconomy\", [\"socio\",\"economy\"]],\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80407821-6e7d-4358-9a8e-84216ead8e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['assistant', ['assist', 'ant']]\n",
      "['assistants', ['assist', 'ant', 's']]\n",
      "['assist', ['assist']]\n",
      "['assisted', ['assist', 'ed']]\n",
      "['assisting', ['assist', 'ing']]\n",
      "['assistance', ['assist', 'ance']]\n",
      "['assistive', ['assist', 'ive']]\n",
      "['assistful', ['assist', 'ful']]\n",
      "['assister', ['assist', 'er']]\n",
      "['unassisted', ['un', 'assist', 'ed']]\n",
      "['coassistance', ['co', 'assist', 'ance']]\n",
      "['coassists', ['co', 'assist', 's']]\n",
      "['overassisting', ['over', 'assist', 'ing']]\n",
      "['entaming', ['en', 'tame', 'ing']]\n",
      "['hoarders', ['hoard', 'er', 's']]\n",
      "['visitorship', ['visit', 'or', 'ship']]\n",
      "['reorganises', ['re', 'organise', 's']]\n",
      "['wargamer', ['war', 'game', 'er']]\n",
      "['encodability', ['en', 'code', 'ability']]\n",
      "['healthy', ['health', 'y']]\n",
      "['buildings', ['build', 'ing', 's']]\n",
      "['socioeconomy', ['socio', 'economy']]\n"
     ]
    }
   ],
   "source": [
    "for instance in test_corpus:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c7ce00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------\n",
      "RESULTS:\n",
      "------------------------------------------\n",
      "Wordpiece: 4\n",
      "BPE: 3\n",
      "Unigram: 2\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "GOLD: assist ant\n",
      "Wordpiece: assistant\n",
      "BPE: assistant\n",
      "Unigram: assistant\n",
      "------------------------------------------\n",
      "GOLD: assist ant s\n",
      "Wordpiece: assistants\n",
      "BPE: assistants\n",
      "Unigram: assistant s\n",
      "------------------------------------------\n",
      "GOLD: assist\n",
      "Wordpiece: assist\n",
      "BPE: assist\n",
      "Unigram: assist\n",
      "------------------------------------------\n",
      "GOLD: assist ed\n",
      "Wordpiece: assisted\n",
      "BPE: assisted\n",
      "Unigram: assiste d\n",
      "------------------------------------------\n",
      "GOLD: assist ing\n",
      "Wordpiece: assisting\n",
      "BPE: assisting\n",
      "Unigram: assisting\n",
      "------------------------------------------\n",
      "GOLD: assist ance\n",
      "Wordpiece: assistance\n",
      "BPE: assistance\n",
      "Unigram: a s sistance\n",
      "------------------------------------------\n",
      "GOLD: assist ive\n",
      "Wordpiece: assist ive\n",
      "BPE: assis tive\n",
      "Unigram: assist i ve\n",
      "------------------------------------------\n",
      "GOLD: assist ful\n",
      "Wordpiece: assist ful\n",
      "BPE: assist ful\n",
      "Unigram: assist ful\n",
      "------------------------------------------\n",
      "GOLD: assist er\n",
      "Wordpiece: assist er\n",
      "BPE: ass ister\n",
      "Unigram: a s sister\n",
      "------------------------------------------\n",
      "GOLD: un assist ed\n",
      "Wordpiece: unass isted\n",
      "BPE: un assisted\n",
      "Unigram: u n assiste d\n",
      "------------------------------------------\n",
      "GOLD: co assist ance\n",
      "Wordpiece: coa ss istan ce\n",
      "BPE: co assistance\n",
      "Unigram: c o a s sistance\n",
      "------------------------------------------\n",
      "GOLD: co assist s\n",
      "Wordpiece: coa ss ists\n",
      "BPE: co assists\n",
      "Unigram: c o assist s\n",
      "------------------------------------------\n",
      "GOLD: over assist ing\n",
      "Wordpiece: over assis ting\n",
      "BPE: over assisting\n",
      "Unigram: over assisting\n",
      "------------------------------------------\n",
      "GOLD: en tame ing\n",
      "Wordpiece: ent aming\n",
      "BPE: ent aming\n",
      "Unigram: ent aming\n",
      "------------------------------------------\n",
      "GOLD: hoard er s\n",
      "Wordpiece: hoard ers\n",
      "BPE: hoard ers\n",
      "Unigram: hoarder s\n",
      "------------------------------------------\n",
      "GOLD: visit or ship\n",
      "Wordpiece: visitors hip\n",
      "BPE: visitor ship\n",
      "Unigram: visitor ship\n",
      "------------------------------------------\n",
      "GOLD: re organise s\n",
      "Wordpiece: reorgan ises\n",
      "BPE: re organ ises\n",
      "Unigram: reorganise s\n",
      "------------------------------------------\n",
      "GOLD: war game er\n",
      "Wordpiece: war game r\n",
      "BPE: war gamer\n",
      "Unigram: wargame r\n",
      "------------------------------------------\n",
      "GOLD: en code ability\n",
      "Wordpiece: enc oda bility\n",
      "BPE: en cod ability\n",
      "Unigram: encod abilit y\n",
      "------------------------------------------\n",
      "GOLD: health y\n",
      "Wordpiece: healthy\n",
      "BPE: healthy\n",
      "Unigram: healthy\n",
      "------------------------------------------\n",
      "GOLD: build ing s\n",
      "Wordpiece: buildings\n",
      "BPE: buildings\n",
      "Unigram: building s\n",
      "------------------------------------------\n",
      "GOLD: socio economy\n",
      "Wordpiece: socio ec ono my\n",
      "BPE: socio economy\n",
      "Unigram: socio econom y\n",
      "------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_wp, count_bpe, count_ug = 0, 0, 0\n",
    "\n",
    "report = \"\"\n",
    "for word, morphs  in test_corpus:\n",
    "    \n",
    "    wp = WP_tokenizer.decode(WP_tokenizer.encode(word).ids).replace(\"#\",'').split()\n",
    "    bpe = BPE_tokenizer.decode(BPE_tokenizer.encode(word).ids).split()\n",
    "    ug = UG_tokenizer.decode(UG_tokenizer.encode(word).ids).split()\n",
    "\n",
    "    if wp==morphs:\n",
    "        count_wp += 1\n",
    "    if bpe==morphs:\n",
    "        count_bpe += 1\n",
    "    if ug==morphs:\n",
    "        count_ug += 1\n",
    "\n",
    "        \n",
    "    report = report + \"GOLD: \" + \" \".join(morphs) + \"\\n\"\n",
    "\n",
    "    report = report + \"Wordpiece: \" + WP_tokenizer.decode(WP_tokenizer.encode(word).ids).replace(\"#\",'') + \"\\n\"\n",
    "\n",
    "    report = report + \"BPE: \" + BPE_tokenizer.decode(BPE_tokenizer.encode(word).ids) + \"\\n\"\n",
    "\n",
    "    report = report + \"Unigram: \" + UG_tokenizer.decode(UG_tokenizer.encode(word).ids) + \"\\n\"\n",
    "    \n",
    "    report = report + \"------------------------------------------\\n\"\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------\")\n",
    "print(\"RESULTS:\")\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Wordpiece:\", count_wp)\n",
    "print(\"BPE:\", count_bpe)\n",
    "print(\"Unigram:\", count_ug)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"\\n\\n\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330384a6-81d6-4740-88c5-f93b67b38716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c90d4-bfe8-4663-8d61-290835f4d5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbaaad3-5faa-4334-be1b-0990d5c82301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
