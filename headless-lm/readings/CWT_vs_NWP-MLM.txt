The Contrastive Weight Tying (CWT) training objective introduced in Godey et al.'s paper represents a significant departure from standard language model training approaches like Masked Language Modeling (MLM) or Next Word Prediction (NWP). In traditional language modeling objectives, models are trained to predict probability distributions over an entire vocabulary for masked or next tokens. This process involves a computationally expensive projection step where hidden representations are mapped to the vocabulary space (often tens of thousands of tokens) through a final classification layer. The standard approach uses weight tying, where the output projection matrix is the transpose of the input embedding matrix, but still requires computing logits for the entire vocabulary. In contrast, CWT eliminates this vocabulary projection entirely. Instead of predicting token probabilities, CWT trains models to reconstruct input embeddings in a contrastive manner. When a token needs to be predicted, rather than generating probabilities across the vocabulary, the model is trained to produce a representation that maximizes similarity with the correct token's embedding while minimizing similarity with other tokens' embeddings that appear in the same batch. This creates what the authors call "Headless Language Models" (HLMs) that operate entirely in the embedding space without needing to map to vocabulary distributions. The contrastive nature comes from using other tokens in the batch as negative examples, pushing the predicted representation away from incorrect tokens and toward the correct one. This approach offers substantial efficiency benefits, reducing both computational complexity and memory requirements, especially with large vocabularies, as the complexity depends on batch size rather than vocabulary size. The paper demonstrates that models trained with CWT not only train faster (up to 20 times less compute required) but also achieve better downstream performance, showing improvements on benchmarks like GLUE and LAMBADA compared to traditional language models trained with similar compute budgets.