sample: single row of data
batch: the number of samples before updating internal model parameters
epoch: the amount of times the training data is fed into the model

warm-up steps: "This usually means that you use a very low learning rate for a set number of training steps (warmup steps). After your warmup steps you use your "regular" learning rate or learning rate scheduler. You can also gradually increase your learning rate over the number of warmup steps." - Ron Schwessigner https://datascience.stackexchange.com/questions/55991/in-the-context-of-deep-learning-what-is-training-warmup-steps 

warmup allows adaptive optimisers (e.g. Adam or RMSProp) to calculate statistics of the gradients. warmup steps prevent "early over-fitting" preventing that the model gets pushed in the wrong direction

pythia procedure: 16 open-source models trained on the exact same data in the same order. ranging from 70M param to 12B. DECODER ONLY.

auto(config/model/tokenizer): access an existing models instance
config: load model parameters such as vocab_size, hidden_layers, hidden_size, num_attention_heads, etc.

accelerator: handles the devices used??


\tasks\pretraining\mlm_headless.py: if self.emb_loss_weight != 0: #wanneer wordt deze aangepast???